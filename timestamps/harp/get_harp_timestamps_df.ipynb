{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and script to interface with recorded Harp Binaries\n",
    "\n",
    "**Inputs:**\n",
    "- Binary files in session folder saved in 'Behavior.harp', and 'SoundCard.harp' subdirectories.<br>\n",
    "- Experimental data .csv file in 'Experimental-data' subdirectory containing trial-level behavioural data output directory from Bonsai workflow.\n",
    "\n",
    "**Key outputs**\n",
    "- trials_df data frame containing a summary of behavioural events in each trial including harp timestamps for dot onset and offset, nose pokes, and audio onset and offset times within each trial. Note that this data set contains redundancy to double check consistency of trial information between this script and the Bonsai output. \n",
    "\n",
    "**Overview** \n",
    "1. Create a general reader for the harp behavior board binaries and another specifically for register 32 of the sound card.\n",
    "2. Align key behavioural events to trials in a pandas data frame in which each row represents one trial. \n",
    "3. Append harp data frame to behavioural summary data frame containing trial-level information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main libraries and define data folder\n",
    "import harp\n",
    "import pandas as pd\n",
    "from harp.model import Model, Register, Access\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.harp_utils as hu\n",
    "import utils.plot_utils as pu\n",
    "\n",
    "#==============================================================================\n",
    "animal_ID = 'FNT099'\n",
    "session_ID = '2024-05-13T11-03-59'\n",
    "\n",
    "# animal_ID = 'FNT107'\n",
    "# session_ID = '2024-08-11T14-01-24'\n",
    "\n",
    "# path behavioural data on ceph repo\n",
    "input_dir = r\"W:\\projects\\FlexiVexi\\behavioural_data\" \n",
    "output_dir = (r\"C:\\Users\\megan\\Documents\\sjlab\\flexible-navigation-task\" +\n",
    "              r\"\\data_analysis\\intermediate_variables\")\n",
    "\n",
    "#==============================================================================\n",
    "\n",
    "# Create reader for behavior.\n",
    "bin_b_path = os.path.join(input_dir, animal_ID, session_ID, \"Behavior.harp\")\n",
    "behavior_reader = harp.create_reader(bin_b_path)\n",
    "\n",
    "# Specify mapping from sound index to reward port\n",
    "soundIdx0 = 14\n",
    "soundIdx1 = 10\n",
    "soundOffIdx = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get harp TTL data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "# Get data frame with state of TTL pulse\n",
    "ttl_state_df = hu.get_ttl_state_df(behavior_reader)\n",
    "\n",
    "# Plot ttl trace\n",
    "t0 = ttl_state_df['timestamp'].iloc[0]\n",
    "\n",
    "pu.plot_ttl_trace(ttl_state_df, t_start=t0, t_end=t0 + 100)\n",
    "plt.title(\"Plot TTL pulses, \" + session_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Align dot times with experimental-data-csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import behavioral data as data frame\n",
    "session_path = os.path.join(input_dir, animal_ID, session_ID)\n",
    "filepath = os.path.join(session_path, 'Experimental-data', session_ID + '_experimental-data.csv')\n",
    "trials_df = pd.read_csv(filepath)\n",
    "\n",
    "# Get dot onset and offset times given by TTL pulses\n",
    "\n",
    "## First dot onset time from software clock (used as a common sense check for inconsistencies with number of TTL pulses on start up)\n",
    "t0 = trials_df['DotOnsetTime'].iloc[0]\n",
    "\n",
    "## Get dot times from TTL pulses\n",
    "[dot_times_ttl, ttl_state_0] = hu.get_dot_times_from_ttl(behavior_reader, t0, return_TTL_state_at_startup=True)\n",
    "print('TTL state upon start-up: ', ttl_state_0)\n",
    "\n",
    "# Append dot onset and offset times given by TTL pulses to trials_df\n",
    "trials_df = pd.concat([trials_df, dot_times_ttl],axis=1)\n",
    "# rename 'DotOnsetTime' and 'DotOffsetTime' columns to 'DotOnsetTime_harp' and 'DotOffsetTime_harp'\n",
    "trials_df.rename(columns = {\n",
    "        'DotOnsetTime_ttl':'DotOnsetTime_harp', \n",
    "        'DotOffsetTime_ttl':'DotOffsetTime_harp'\n",
    "        }, \n",
    "    inplace = True)\n",
    "\n",
    "# Common sense check that the logic of aligning the TTL pulses is working as expected.\n",
    "# Check dot onset and times from software clock TTL pulses are consistent, given by:\n",
    "# - DotOnsetTime = dot onset time from software clock\n",
    "# - DotOnsetTime_ttl = dot onset time from TTL pulses\n",
    "# - DotOffsetTime = dot offset time from software clock\n",
    "# - DotOffsetTime_ttl = dot offset time from TTL pulses\n",
    "\n",
    "trials_df[['TrialStart', 'DotOnsetTime', 'DotOffsetTime', 'DotOnsetTime_harp', 'DotOffsetTime_harp']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all poke events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the behavior harp stream, Digital Input states for the nosepoke timestamps and IDs. Drop DI3 <-- What's DI3??\n",
    "all_pokes = behavior_reader.DigitalInputState.read()\n",
    "\n",
    "all_pokes.drop(columns=['DI3','DIPort2'],inplace = True) # remove all nose pokes to dummy port\n",
    "#all_pokes.reset_index(inplace=True)\n",
    "\n",
    "# Show resulting data frame\n",
    "all_pokes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all audio events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sound card binary data (register 32) and show resulting dataframe\n",
    "bin_sound_path = os.path.join(input_dir, animal_ID, session_ID, \"SoundCard.harp\",\"SoundCard_32.bin\")\n",
    "\n",
    "# the explicitly defined model will be deprecated or redundant in future\n",
    "model = Model(device='Soundcard', whoAmI=1280,firmwareVersion='2.2',hardwareTargets='1.1',registers={'PlaySoundOrFrequency': Register(address=32, type=\"U16\", access=Access.Event)})\n",
    "sound_reader = harp.create_reader(model, keep_type=True)\n",
    "\n",
    "# Read the harp sound card stream, for the timestamps and audio ID\n",
    "all_sounds = hu.get_all_sounds(sound_reader, bin_sound_path)\n",
    "\n",
    "# Show dataframe (maybe)\n",
    "all_sounds.head(10)\n",
    "\n",
    "all_sounds_test = all_sounds[all_sounds['PlaySoundOrFrequency'] != soundOffIdx]\n",
    "all_sounds_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get trial start times in harp clock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_start_times(stage, **kwargs):\n",
    "\n",
    "    # NOTE: add optional argument t0 (or even all trial start times) to check for consistency \n",
    "    # between software clock and harp clock\n",
    "\n",
    "    if stage == 4:\n",
    "        dot_onset_times = kwargs.get('dot_onset_times')\n",
    "        if dot_onset_times is None:\n",
    "            raise ValueError(\"Stage 4 requires 'dot_onset_times' argument.\")\n",
    "        # Process dot onset and offset times\n",
    "        trial_start_times = dot_onset_times\n",
    "        return trial_start_times\n",
    "\n",
    "    elif stage == 5:\n",
    "        bin_sound_path = kwargs.get('bin_sound_path')\n",
    "        sound_reader = kwargs.get('sound_reader')\n",
    "        if bin_sound_path is None or sound_reader is None:\n",
    "            raise ValueError(\"Stage 5 requires 'bin_sound_path' and 'sound_reader' arguments.\")\n",
    "        # Derive the sound of all audio events\n",
    "        sound_of_all_audio_events = sound_reader(bin_sound_path)\n",
    "        print(f\"Processing stage 5 with sound_of_all_audio_events: {sound_of_all_audio_events}\")\n",
    "        all_sounds = hu.get_all_sounds(sound_reader, bin_sound_path)\n",
    "        # remove events for playing silence\n",
    "        all_sounds = all_sounds[all_sounds['PlaySoundOrFrequency'] != soundOffIdx]\n",
    "        # take trial start time is the start of each audio (since there is one audio cue per trial)\n",
    "        trial_start_times = all_sounds['timestamp']\n",
    "        # NOTE: might need to introduce a check for trials in stage 5 where the sound starts playing \n",
    "        # but the trial is not completed. In this case we should discard the last trial.\n",
    "        return trial_start_times\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stage. Only stages 4 and 5 are supported.\")\n",
    "\n",
    "# check training stage specified in 'trials_df'\n",
    "stage = trials_df['TrainingStage'].iloc[0]\n",
    "\n",
    "# get trial start times for the specified stage\n",
    "if stage == 4:\n",
    "    dot_onset_times = trials_df['DotOnsetTime_harp']\n",
    "    trial_start_times = get_trial_start_times(4, dot_onset_times=dot_onset_times)\n",
    "elif stage == 5:\n",
    "    trial_start_times = get_trial_start_times(5, bin_sound_path=bin_sound_path, sound_reader=sound_reader)\n",
    "\n",
    "# Append trial start times to trials_df\n",
    "trials_df = pd.concat([trials_df, trial_start_times.rename('TrialStart_harp')],axis=1)\n",
    "\n",
    "trials_df[['TrialStart', 'TrialStart_harp', 'DotOnsetTime', 'DotOffsetTime', 'DotOnsetTime_harp', 'DotOffsetTime_harp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Align poke events with trials**\n",
    "\n",
    "Get data frame with port choice ID and timestamp for each trial, where the port choice is taken as the first nose poke within the response window (between dot offset and trial end). If the trial is aborted, the port ID and timestamp are both taken as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data frame with port choice ID and timestamp for each trial\n",
    "port_choice_df = hu.get_port_choice(trials_df, behavior_reader)\n",
    "\n",
    "# Append port choice to trials_df\n",
    "trials_df = pd.concat([trials_df, port_choice_df],axis=1)\n",
    "\n",
    "# Show port choice data frame\n",
    "port_choice_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Align sound events to trials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_trial_sounds(trial_start_times, sound_reader, bin_sound_path, OFF_index=18):\n",
    "    # Read the harp sound card stream, for the timestamps and audio ID\n",
    "    all_sounds = hu.get_all_sounds(sound_reader, bin_sound_path)\n",
    "\n",
    "    # Create lists to store the poke IDs and timestamps for all trials\n",
    "    ON_S, OFF_S, ID_S = [], [], []\n",
    "\n",
    "    # Iterate through trial start times and extract data from harp stream\n",
    "    for i, start_time in enumerate(trial_start_times):\n",
    "        if i < len(trial_start_times) - 1:\n",
    "            end_time = trial_start_times[i + 1]\n",
    "        else:\n",
    "            end_time = start_time + 100  # 100 seconds after the last trial start time\n",
    "\n",
    "        # Extract events that occur within the time range of this trial\n",
    "        trial_events = all_sounds[(all_sounds.Time >= start_time) & (all_sounds.Time <= end_time)]\n",
    "\n",
    "        # Create trial lists for sounds this trial\n",
    "        ON, OFF, ID = [], [], []\n",
    "        for _, sound in trial_events.iterrows():\n",
    "            event_time = sound.Time\n",
    "            sound = sound[['PlaySoundOrFrequency']]\n",
    "            sound = int(sound.iloc[0])\n",
    "\n",
    "            # Find audio IDs from the value. Only find ID for OFFSET\n",
    "            if sound != OFF_index:\n",
    "                ON.append(event_time)\n",
    "                ID.append(sound)\n",
    "            else:\n",
    "                OFF.append(event_time)\n",
    "\n",
    "        ON_S.append(ON)\n",
    "        OFF_S.append(OFF)\n",
    "        ID_S.append(ID)\n",
    "        \n",
    "    trial_sounds_df = pd.DataFrame({'AudioCueStart_harp': ON_S, 'AudioCueEnd_harp': OFF_S, 'AudioCueIdentity_harp': ID_S})  # Create dataframe from all nosepoke events\n",
    "\n",
    "    return trial_sounds_df\n",
    "\n",
    "# Get data frame with sound ID and timestamp for each trial\n",
    "trial_sounds_df = parse_trial_sounds(trials_df['TrialStart_harp'], sound_reader, bin_sound_path)\n",
    "\n",
    "# Append sound ID to trials_df\n",
    "trials_df = pd.concat([trials_df, trial_sounds_df],axis=1)\n",
    "\n",
    "# Show sound data frame\n",
    "trial_sounds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trials_df AudioCueStart and ChoicePort is as expected\n",
    "trials_df[\n",
    "    [\n",
    "        'TrialStart',\n",
    "        'TrialStart_harp',\n",
    "        'TrialCompletionCode',\n",
    "        'ChoicePort',\n",
    "        'ChoiceTimestamp',\n",
    "        'AudioCueStart_harp',\n",
    "        'AudioCueEnd_harp',\n",
    "        'AudioCueIdentity'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save trials_df dataframe for further analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trials_df as a .pkl file to be used for further analysis\n",
    "session_output_dir = os.path.join(output_dir, animal_ID, session_ID)\n",
    "trials_df.to_pickle(os.path.join(session_output_dir, animal_ID + '_' + session_ID + '_trial_data_harp.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get photodiode data -- work in progress!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab photodiode data\n",
    "photodiode = behavior_reader.AnalogData.read()\n",
    "photodiode_signal = photodiode.AnalogInput0\n",
    "\n",
    "plt.figure()\n",
    "# Set the trial number you want to see here\n",
    "set_trial_number = False\n",
    "if set_trial_number:\n",
    "    trial_number = 1\n",
    "    ist = 1*trial_number\n",
    "    ie = trial_number+3\n",
    "else:\n",
    "    ist = 0\n",
    "    ie = -1\n",
    "\n",
    "\n",
    "# Find Timestamps for this trial\n",
    "t_start = trials_on.index[ist]\n",
    "t_end = trials_on.index[ie]\n",
    "\n",
    "# plot the diode trace, draw points at the TTL onsets. Restrict plot to chosen trial (2 seconds either side of TTL)\n",
    "plt.plot(photodiode_signal, label = 'Photodiode')\n",
    "plt.plot(trials_on,'o', label = 'TTL ON')\n",
    "plt.plot(trials_off,'o', label = 'TTL OFF')\n",
    "plt.xlim([t_start-2,t_end-2])\n",
    "plt.xlabel('Time (Harp timestamps)')\n",
    "plt.ylabel('AU')\n",
    "plt.legend(loc = 'upper right')\n",
    "# total_trials = trials_on.shape[0] /3\n",
    "# print(f'Total trials = {total_trials}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
