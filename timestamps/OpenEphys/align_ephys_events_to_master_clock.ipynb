{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align ephys events to a master clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose session to analyze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from open_ephys.analysis import Session\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root_dir = Path('/ceph/sjones/projects/FlexiVexi/behavioural_data')\n",
    "OUTPUT = Path(\"/ceph/sjones/projects/FlexiVexi/Data Analysis/intermediate_variables\")\n",
    "\n",
    "\n",
    "# (Example session with both TTL and heartbeat events):\n",
    "animal_ID = 'FNT103'\n",
    "session_ID = '2024-08-26T14-37-42'\n",
    "#animal_ID = 'FNT104'\n",
    "#session_ID = '2024-06-12T09-37-41'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load recording**\n",
    "* Load recording object\n",
    "* Load continuous.dat from stream 'NI-DAQmx-103.PXIe-6341'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_record_node_path(root_folder):\n",
    "    # Traverse the directory tree\n",
    "    for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "        # Check if 'settings.xml' is in the current directory\n",
    "        if 'settings.xml' in filenames:\n",
    "            return dirpath\n",
    "        else:\n",
    "            print('No recording found')\n",
    "\n",
    "def get_session_path(root_folder):\n",
    "    # Traverse the directory tree\n",
    "    folder_one_level_up = None\n",
    "    for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "        # Check if any file ends with 'settings.xml'\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('settings.xml'):\n",
    "                # Get the folder one level up\n",
    "                folder_one_level_up = os.path.dirname(dirpath)\n",
    "                return folder_one_level_up\n",
    "    if folder_one_level_up is None:\n",
    "        print('No recording found')\n",
    "\n",
    "\n",
    "\n",
    "# Find the path to the recording session\n",
    "session_folder = input_root_dir / animal_ID / session_ID\n",
    "print(session_folder)\n",
    "ephys_session_path = get_session_path(session_folder)\n",
    "print(ephys_session_path)\n",
    "\n",
    "# load recording\n",
    "session = Session(ephys_session_path)\n",
    "recording = session.recordnodes[0].recordings[0]\n",
    "\n",
    "# Get pandas data frame of continuous.dat on stream 'NI-DAQmx-103.PXIe-6341'\n",
    "continuous_data = recording.continuous[1].get_samples(start_sample_index=0, end_sample_index=40000)\n",
    "continuous_data = pd.DataFrame(continuous_data)\n",
    "continuous_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each session will have a series of record nodes. For us, just one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each recording object has the following fields:\n",
    "\n",
    "continuous : continuous data for each subprocessor in the recording  \n",
    "spikes : spikes for each electrode group  \n",
    "events : Pandas DataFrame of event times and metadata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pandas data frame of continuous.dat on stream 'NI-DAQmx-103.PXIe-6341'\n",
    "continuous_data = recording.continuous[1].get_samples(start_sample_index=0, end_sample_index=40000)\n",
    "print(continuous_data)\n",
    "continuous_data = pd.DataFrame(continuous_data)\n",
    "continuous_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous data for each recording is accessed via the .continuous property of each Recording object. This returns a list of continuous data, grouped by processor/sub-processor. For example, if you have two data streams merged into a single Record Node, each data stream will be associated with a different processor ID. If you're recording Neuropixels data, each probe's data stream will be stored in a separate sub-processor, which must be loaded individually.\n",
    "\n",
    "Each continuous object has four properties:\n",
    "\n",
    "samples - a numpy.ndarray that holds the actual continuous data with dimensions of samples x channels. For Binary, NWB, and Kwik format, this will be a memory-mapped array (i.e., the data will only be loaded into memory when specific samples are accessed). \n",
    "\n",
    "sample_numbers - a numpy.ndarray that holds the sample numbers since the start of acquisition. This will have the same size as the first dimension of the samples array \n",
    "\n",
    "timestamps - a numpy.ndarray that holds global timestamps (in seconds) for each sample, assuming all data streams were synchronized in this recording. \n",
    "This will have the same size as the first dimension of the samples array  \n",
    "\n",
    "metadata - a dict containing information about this data, such as the ID of the processor it originated from.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading event data\n",
    "\n",
    "Event data for each recording is accessed via the `.events` property of each `Recording` object. This returns a pandas DataFrame with the following columns:\n",
    "\n",
    "- `sample_number` - the sample index at which this event occurred\n",
    "- `timestamps` - the global timestamp (in seconds) at which this event occurred (defaults to -1 if all streams were not synchronized)\n",
    "- `channel` - the channel on which this event occurred\n",
    "- `processor_id` - the ID of the processor from which this event originated\n",
    "- `stream_index` - the index of the stream from which this event originated\n",
    "- `state` - 1 or 0, to indicate whether this is a rising edge or falling edge event. \n",
    "\n",
    "What is a `line` here? We have 1 and 4 of those,and they both look like they oscillate as a fixed square wave with different periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_4 = recording.events[(recording.events[\"line\"]==4) & (recording.events['processor_id']==103)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg =  1\n",
    "end = 10\n",
    "plt.plot(line_4['timestamp'][beg:end],line_4['state'][beg:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether events exist on line 4\n",
    "events = recording.events\n",
    "\n",
    "# get unique elements of stream_name\n",
    "data_streams = list(set(events['stream_name']))\n",
    "\n",
    "print(len(data_streams))\n",
    "print(data_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = recording.events\n",
    "\n",
    "\n",
    "TTL_pulses = event_df[(event_df['stream_name'] == 'PXIe-6341') & (event_df['line'] == 4)]\n",
    "TTL_pulses = TTL_pulses.reset_index(drop=True)\n",
    "TTL_pulses\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(TTL_pulses['timestamp'], TTL_pulses['state'])\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_xlabel('Timestamp (s)')\n",
    "ax.set_ylabel('TTL in PXIe board')\n",
    "fig.suptitle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTL_pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether sync line can be added\n",
    "\n",
    "# Sync line corresponding to heartbeat signal of ephys clock (1 pulse per second of duration 0.5 seconds). \n",
    "# Use this as the master clock (set main = True).\n",
    "recording.add_sync_line(1,                          # 'Heartbeat' signal line number\n",
    "                        100,                        # processor ID\n",
    "                        'ProbeA',                   # stream name\n",
    "                        main=True)                  # use as the main stream\n",
    "\n",
    "\n",
    "# Sync line corresponding to TTL pulses\n",
    "recording.add_sync_line(1,                          # TTL line number\n",
    "                        102,                        # processor ID\n",
    "                        'PXIe-6341',                # stream name\n",
    "                        main=False)                 # synchronize to main stream\n",
    "\n",
    "#THE PROCESSOR ID IS 102, NOT 103??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording.compute_global_timestamps(overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = recording.events\n",
    "event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTL_pulses = event_df[(event_df['stream_name'] == 'PXIe-6341') & (event_df['line'] == 4)]\n",
    "TTL_pulses = TTL_pulses.reset_index(drop=True)\n",
    "TTL_pulses\n",
    "\n",
    "# save the TTL pulses to a csv file\n",
    "#TTL_pulses.to_csv('TTL_pulses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTL_pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(TTL_pulses['global_timestamp'], TTL_pulses['state'])\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement and verify alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timestamps.harp.get_harp_timestamps_df import harp_session\n",
    "from timestamps.OpenEphys.open_ephys_utils import openephys_session\n",
    "\n",
    "animal_ID = 'FNT103'\n",
    "session_ID = '2024-08-26T14-37-42'\n",
    "#animal_ID = 'FNT104'\n",
    "#session_ID = '2024-06-12T09-37-41'\n",
    "\n",
    "harp = harp_session(animal_ID, session_ID)\n",
    "harp.read_ttl()\n",
    "harp.plot_ttl(100)\n",
    "\n",
    "oe = openephys_session(animal_ID, session_ID)\n",
    "oe.read_TTLs()\n",
    "oe.plot_TTLs()\n",
    "oe.sync_harp_ttls()\n",
    "\n",
    "harp.ttl_state_df['global_timestamp'] = oe.tm.get_pxie_timestamp(harp.ttl_state_df['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax =  plt.subplots()\n",
    "ax.plot(oe.TTL_pulses['global_timestamp'], oe.TTL_pulses['state'])\n",
    "harp.ttl_state_df['global_timestamp'] = oe.tm.get_pxie_timestamp(harp.ttl_state_df['timestamp'])\n",
    "ax.plot(harp.ttl_state_df['global_timestamp'], harp.ttl_state_df['state'])\n",
    "ax.set_xlim((0+inc, 100+inc))\n",
    "fig.suptitle(f'{0+inc} to {100+inc}')\n",
    "inc +=  100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
